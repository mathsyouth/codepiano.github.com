---
layout: post
keywords: cws, chinese word segmentation
description: 中文分词基础知识
title: "[经典转载]对于bakeoff-3的简单综述"
categories: [NLP]
tags: [cws, chinese-word-segmentation]
group: archive
icon: comment
---
{% include mathsyouth/setup %}


### 对于bakeoff-3的简单综述

SIGHAN举办的第三届国际中文处理竞赛（the third international Chinese language processing Bakeoff， http://www.sighan.org/bakeoff2006/）在2006年4月17日到5月19 日举行。本项活动的前两届分别在2003年和2005年举行，原名叫做interntional Chinese word segmentation Bakeoff。今年加入了命名实体的识别，所以竞赛也改名了。至于这项国际评测活动为什么叫bakeoff，似乎没有一个官方的说法，bakeoff是bake off这个短语的复合词，意为烘烤，可能就是意味着参赛者的努力处于煎熬中吧。反正，我参赛期间的 msn昵称就是bakeoff 开始bake off。首先说下结果，bakeoff-3的详细结果列在URL: http://sighan.cs.uchicago.edu/bakeoff2006/longstats.html 尽管这个地址没有公开，但是其实可以使用google搜到的。本次比赛的中文分词项目对于我代表的微软亚洲研究院来说是一个辉煌的胜利，因为MSRA本身是一个语料的提供方，所以没有参加自己提供语料的分词项目，而参加了剩余的3个语料的6项赛事，每个语料分封闭和开放两项。在官方结果中，我们的系统在六项中获得了4个第一，2个第三。不过，事后我发现，我们处于第三的UPUC2006的开放测试的结果，我的文件编码出了错，这项的实际成绩，我们比官方结果中的第一名F score高0.9。我们另外处于第三的是CityU2006的封闭测试，第一名和第二名都是台湾中研院同一个系统不同参数的两个结果，成绩一样。在我们自己的MSRA2006语料上，我用同样的系统跑了一遍，在封闭测试上，我们是第二，在开放上我们依然是第一，比官方最好成绩高0.3。基本上来说，没什么意外的话，我们囊括了几乎的本次比赛中的分词项目的所有的第一名。我在7月sighan-2006今年7月开会前，已经通过多种渠道读到了今年的大部分参赛的结果的系统描述报告。上个月乘着开完会拿到光盘，把所有的论文读完 了。所以，我想现在可以给出个较为全面的综述了。我统计了所有至少获得第一名的系统 
，这样的系统，包括我们的，有5个。在总共8个第一中，我们包揽了4个，剩下4个分别各被一家所夺走。总共有 24项前三名的名次，其中的19项也被这5家包揽。因此，本次比赛的结果即使用前三名来统计，也是非常集中的，同样集中上述的5家参赛单位。为了描述方便，下面称这5家参赛单位为top-5。所有这些论文的参考文献如下： 

\bibitem{Zhao:2006} 
Hai Zhao, Chang-Ning Huang and Mu Li. 2006. An Improved Chinese Word 
Segmentation System with Conditional Random Field. {\em Proceedings of the 
Fifth SIGHAN Workshop on Chinese Language Processing}, 108-117. Sidney, 
Australia. 

\bibitem{Wang:2006} 
Xinhao Wang; Xiaojun Lin; Dianhai Yu; Hao Tian; Xihong Wu. 2006. Chinese Word 
Segmentation with Maximum Entropy and N-gram Language Model. {\em Proceedings 
of the Fifth SIGHAN Workshop on Chinese Language Processing}, 108-117. Sidney, 
Australia. 

\bibitem{Jacobs:2006} 
Aaron J. Jacobs; Yuk Wah Wong. 2006. Maximum Entropy Word Segmentation of 
Chinese Text. {\em Proceedings of the Fifth SIGHAN Workshop on Chinese Language 
Processing}, 108-117. Sidney, Australia. 

\bibitem{Tsai:2006} 
Richard Tzong-Han Tsai; Hsieh-Chuan Hung; Cheng-Lung Sung; Hong-Jie Dai; 
Wen-Lian Hsu. 2006. On Closed Task of Chinese Word Segmentation: An Improved 
CRF Model Coupled with Character Clustering and Automatically Generated 
Template Matching. {\em Proceedings of the Fifth SIGHAN Workshop on Chinese 
Language Processing}, 108-117. Sidney, Australia. 

\bibitem{Liu:2006} 
Wu Liu; Heng Li; Yuan Dong; Nan He; Haitao Luo; Haila Wang. 2006. France 
Telecom R\&D Beijing Word Segmenter for Sighan Bakeoff 2006. {\em Proceedings 
of the Fifth SIGHAN Workshop on Chinese Language Processing}, 108-117. Sidney, 
Australia. 

Top-5的系统信息：
MSRA/自然语言计算组
第一名成绩：UPUC-C/CityU-O/AS-O/AS-C 
学习模型：CRF 特征改进自Low and Ng 
北京大学/机器感知国家实验室 
第一名成绩：MSRA-C 
学习模型ME 
特征拷贝自Low and Ng 
台湾中研院/智能Agent系统实验室 
第一名成绩：CityU-C 
学习模型ME 
特征使用聚类算法重现Low and Ng 
法国电信北京研发中心 
第一成绩：MSRA-O 
学习模型Gao method（language model）/ME 
特征类似Low and Ng 
德州大学Austin分校/语言学系 
第一名成绩：UPUC-O 
学习模型ME 
特征拷贝自Low and Ng 

上面提到的Low and Ng是如下的参考文献 

\bibitem{Low:2005} 
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo 2005. A Maximum Entropy Approach to 
Chinese Word Segmentation. {\em Proceedings of the Fourth SIGHAN Workshop on 
Chinese Language Processing}, 161-164. Jeju Island, Korea. 

这是2005年赛事上取得最多第一的系统，在参加的全部4个开放测试中累计获得3个第一，1个第二。因此，我愿意说，bakeoff-3的结果不是本届参赛者的胜利，而是bakeoff-2的参赛者Low and Ng的胜利。让我们稍微回顾一下bakeoff-2，Low and Ng几乎垄断了开放测试 
，Tseng等人则几乎垄断了封闭测试。然而，重要的是，这两个参赛者，用的都是基于字标引的机器学习方法。这一方法的开创者是bakeoff-1的Xue: 

\bibitem{Xue:2003a} 
Nianwen Xue. 2003. Chinese Word Segmentation as Character Tagging. {\em 
Computational Linguistics and Chinese Language Processing}, Vol. 8(1): 29-48. 

\bibitem{Xue:2002} 
Nianwen Xue and S. P. Converse. 2002. Combining Classifiers for Chinese Word 
Segmentation. {\em Proceedings of the First SIGHAN Workshop on Chinese Language 
Processing}, 57-63. 

\bibitem{Xue:2003b} 
Nianwen Xue and Libin Shen. 2003. Chinese Word Segmentation as LMR Tagging. In 
{\em Proceedings of the 2nd SIGHAN Workshop on Chinese Language Processing, in 
conjunction with ACL'03}, 176-179. Sapporo, Japan 

依此追溯，bakeoff至今的全部辉煌其实是Xue的方法论在中文分词上的胜利。 


分词现在没有统一的唯一评价标准。 不同的研究机构对什么是中文中的词的定义不一致， 北大有自己的分词体系， 中科院的和北大的很接近， 但是还是有不一致的地方， 微软的分词标准和北大的就有很大的不同， MSRA提供的分词语料里面很多命名实体当作一个词看待。  

由于没有唯一的分词标准， 所以SIGHAN评价的时候就是让几个有影响力的研究机构提供自己切好的语料作为训练数据和测试数据， 各个参赛者参加把自己的切词器指定的数据上分别进行训练然后测试， 得到不同的 precision/recall。 

一个指定的训练／测试数据就是一个 track,  参赛者可以选择参加不同 
的 track。 

分词的标准首先涉及语言学上怎样定义一个中文字符串是词，这个标准通常由语言学家来定义， 但是由于中文的词和短语之间的界限不明晰， 所以即使在语言学家之间什么是词也同样有争议， 不同的机构可能指定不同的语言学标准。  

但是考虑到中文信息处理的需要， 计算机切词中的 '词' 并不是严格按照语言学标准进行的， 毕竟词是切给计算机用的， 而不是给人看的， 所以在NLP中定义中文的词的时候往往以语言学上的准则为指南， 兼顾计算机处理的需要。  比如  '一个' 在语言学上是个数量结构， 应该分为'一' 和 '个' 两个词， 但是在计算机里面当作一个词对待。 所以严格的说计算机切分出来的词不叫词， 而叫 ‘切分单位'. 参看陆俭明老师的 《现代汉语语法研究教程》 第十一节：  中文信息处理中的语法研究


### 对于bakeoff-3的简单综述-2 

好了,基本的结论可以有一个了, 
要想取得 state-of-the-art的中文分词性能,需要的要素其实只有两个: 
1. 字标引(character-based tagging) 
2. 一个类似于CRF/ME的学习模型 
至于特征,好像Low and Ng以及今年的结果已经很充分了. 
据我的观察,自从基于语料定义分词标准以及机器学习意义下的分词算法诞生以来,分词的特征体系似乎渐渐形成了两个主流: 
一个是MSRA的Gao Jianfeng(我在交大的师兄)的特征体系,代表作 

\bibitem{Gao:2005} 
Jianfeng Gao, Mu Li, Andi Wu and Chang-Ning Huang. 2005. Chinese Word Segmentation and Named Entity Recognition: A Pragmatic Approach. Computational Linguistics, Vol. 31(4): 531-574. 

这个特征体系在去年的Tseng的封闭测试系统中得到了充分的展示. 并且在今年法国电信的系统中也得到了发挥. 法国电信的系统在MSRA2006语料的开放测试中获得第一不是偶然的. 因为当初Jianfeng师兄的特征系统本来就是基于MSRA标准的语料作语言特征统计而同步开发的, 
法国电信使用了Gao的特征外加TBL大法从而获得第一其实是意料之中(虽然他们的成绩其实比我们的还是低一点). 

另外一个特征系统是Low and Ng在去年,其实是在前年的EMNLP-04上提出来来的.在大家都知道的n-gram特征之外,Ng引入了标点,字母,时间/日期,数字的区分. 
虽然简单,或许这是最近3年语言特征的最大突破了吧.中文分词和其他的语言学习不同,因为它是最基本的和第一位的语言处理,根本无法使用丰富的语言现象来进行辅助性操作.比如,典型的,使用POS特征,这在分词信息学习中根本是不能想象的,除非有谁能够给汉字分配POS. 

下面我简要的回答这个问题,为什么这次MSRA的系统差点把所有的第一一锅端了. 


### 对于bakeoff-3的简单综述-3(完)

原因在于，我们引入了复杂的标注集来标注字在词中的位置。说来话长，理论上来说，切分问题是一个二值分类问题，切还是不切（哈姆雷特：To be, or not to be: that is the question，：））。但是第一个关于字标注的工作，

\bibitem{Xue:2003a}
Nianwen Xue. 2003. Chinese Word Segmentation as Character Tagging. {\em Computational Linguistics and Chinese Language Processing}, Vol. 8(1): 29-48.

并不是将其作为二值标注问题来处理的，而是作为4值标注问题。具体来说，就是将字在一个词中间的位置分为4类：词首B，词尾 E，词中M，单字词S。作为二值标注问题，可能最开始由Peng Fuchun在coling-04引入。为什么会出现这样一种“退步”？原因在于计算负载上，Xue用的是最大熵(MaxEnt)，而Peng用的 CRFs。显然的，CRF的计算负载一般要比MaxEnt高一个数量级，而且，CRF上用4个标注的学习代价比2个要高一倍不止。实际上，后续的CRF模型，只要是用于分词的，能够不用2个词位标注的学习非常非常罕见。

但是，但是，我们工作的要点就是要引入更多的词位标注来进行更为精确的词位学习。具体来说，我们进一步引入了2个额外的词位，B2,B3，分别代表多字词中第二个和第三个字的非词尾的位置。这样子，我们在CRFs中相当于前所未有的用到了6个词位标注。与此同时，我们大大降低了特征模版的数量。

下面对比三个特征模版集的不同，分别针对2，4，6标注集。我们分别用T, X, Z表示Tsai et al. 2006，Xue:2003/Low:2005, Zhao:2006用到的针对这三个标注集的特征模版：

　　C-2, C-1, C0, C1, C2, C-2C-1, C-1C0, C0C1, C1C2, C-1C1, C-2C0, C-3C-1
T:   x    x    x   x        x       x     x             x     x      x
X:   x    x    x   x   x    x       x     x      x      x
Z:        x    x   x                x     x             x

可以看到，Tsai, Xue/Low都要用到10个特征模版，而Zhao只需要6个。从特征模版来看，Tsai的模版涉及到5个字的上下文窗口：C-3到C1, Xue/Low涉及到5个字的上下文窗口：C-2到C2，Zhao涉及到三个字的上下文窗口：C-1到C1。这似乎显示Zhao的模版学习的上下文窗口宽度不足。但是，如果我们观察，不重复的使用词位标注集标注一个尽可能长的词
2-tag, 两字词，BE
4-tag,三字词，BME
6-tag,五字词，BB2B3ME
也就是说，6-tag通过标注集而不是模版集足以保证学习5字上下文窗口。

如果把我们的目标归结为寻求一种性能最好的n-gram特征和标注集搭配。我们将会发现6-tag+6-ngram会是最佳的。为此，我在同样的CRF模型上用Tsai的2-tag标注和他们的n-gram特征模版（实际上，我发现在所有用于2标注集的特征模版中，他们的这组是性能最好的），Xue/Low 的4-tag标注和他们的n-gram特征模版，进行了性能评估。下面是在Bakeoff-3数据上的F-score结果比较

           AS    CityU    CTB    MSRA
2-tag+Tsai     .946       .962           .924          .946
4-tag+Xue     .952       .967           .930          .955
6-tag+Zhao   .954       .969           .932          .961

那么是什么阻止我们使用更多的标注集呢，可能是训练计算的时间太长。下面是训练时间（单位是分）的比价，所有的数据都在一致的计算环境中获得
          AS    CityU    CTB    MSRA
2-tag+Tsai    112        52               16           35
4-tag+Xue     206       79                28           73
6-tag+Zhao   402       146              47           117
可以看到，6-tag比4-tag基本上要多花一倍的训练时间，比2-tag要多花2-3倍的训练时间。

再看训练内存使用(单位Giga bytes)

          AS    CityU    CTB    MSRA
2-tag+Tsai     5.4        2.4              0.9          1.8
4-tag+Xue     6.6        2.8              1.1           2.2
6-tag+Zhao   6.4        2.7              1.0           2.1

注意到，6-tag用的内存甚至比4-tag系统还要少！所以，去买一台快点的计算机跑分词吧，因为我们不需要一个复杂的系统来做分词。简单的6-tag+6-ngram就够了。

最后，我说一下字符类型特征和开放测试的问题。

所谓字符类型特征，就是标记字符是正常的汉字，还是拉丁字母，阿拉伯数字，还是标点符号的特征。Bakeoff-3明确禁止在封闭测试中使用这一特征。不幸的是，很多人，包括我，没有注意到网页上的这一条款。

下面是bakeoff-3在分词封闭测试中至少获得前三名的参赛者及其成绩（F-score）列表，总共有6个参赛者。

Participant   AS    CityU   CTB    MSRA
(Site ID)
Zhu(1)       0.944  0.968   0.927  0.956
Carpenter(9) 0.943  0.961   0.907  0.957
Tsai(15)     0.957  0.972   /      0.955
Zhao(20)     0.958  0.971   0.933  /
Zhang(26)    0.949  0.965   0.926  0.957
Wang(32)     0.953  0.970   0.930  0.963

------------------------------------------
Best closed  0.958  0.972   0.933  0.963

其 中，Zhu, Tsai, Zhao用了字符类型特征，Zhu和Zhao是直接从预先指定的字符类型列表中构造这一特征。Tsai用了一个别出心裁的方法，通过几条预定义的目标函数进行聚类来获得字符类型列表。当然，他们重新“发现”了其实事先就知道的不同的字符类型。也许，他们的这种做法才是最没有犯规的。但是，这种做法的实用价值是值得商榷的，是否有为了评估而评估之嫌？

关于字符类型特征的作用，我做过事后的经验性评估。我的结论是：字符类型特征在某型情形的确是发挥作用的，也就是对于性能的提升的确有帮助，但是，这种性能增进强烈的依赖于两个方面，第一个是所学习的文本的全半角是否统一，如果某些字母，数字，标点是半 角，某些是全角。那么，字符类型特征会有较大的改进作用。反之，如果训练和测试语料中的全半角是基本一致的，也就是基本都是半角或者都是全角，那么，该特 征将不起什么作用。第二个因素是训练语料的规模，不管全半角是否一致，该特征在大规模训练语料下对于性能改进微乎其微，相反，对于小规模的训练语料的性能 改进较为可观。简而言之，字符类型特征不会导致性能的实质性增长。从这个角度来说，规定在封闭测试中禁止或者使用这一特征都讲得通。

这里顺便说一 下，Wang(32)在MSRA上封闭测试的结果，他们使用了语言模型概率和MaxEnt输出联合解码的方法。他们的方法依赖于一个分配这两种输出的权 参数。不幸的是，根据他们的报告，在微软语料上的这一参数调整来自Bakeoff-2005的微软语料。而Bakeoff-2006微软训练语料只是 2005的相应语料的一个子集（正是我建议黄老师取一半的2005语料提交为2006语料的），这样算不算引入了外部语言资源的信息？也许这个可以解释他们的这个成绩为什么比第二名高那么多吧

关于开放测试
目前为止，我只谈了封闭测试。为什么我不愿意将开放测试作为另外一个重点。原因就是在开放测试中的方法论其实相当的贫乏和单调，尽管Bakeoff的开放测试允许一切额外的语言资料和外部知识。
两种显而易见的资源已经被往届的Bakeoff参赛者集成到分词学习中了。一个是词典，一个是额外的切分语料。这一工作依然由Low and Ng在Bakeoff-2005完成。Bakeoff-3(2006)有两个值得一提的工作，一个是引入了第三种开放资源，词性标注(POS tagging)结果。这一工作在，
\bibitem{Wang and Shi:2006}
Mengqiu Wang and Yanxin Shi, Using Part-of-Speech Reranking to Improve Chinese Word Segmentation, SIGHAN-5.

另外一个试集成方法的改进。这个依然是我们的工作，我们建议直接引入其他的切分器的输出作为特征。我们称之为辅助切分器方法。这样，词典，附加语料都可以统 一为辅助切分器，前者可以用最大匹配辅助切分器，后者可以用在附加语料上训练出来的切分器。这样做的优势，除了方法统一外，还有助于应用到CRF学习上， 因为额外语料方法会大大增加训练时间。这对CRF训练来说是一个灾难。额外的，我们还证明，辅助的命名实体识别输出也是对分词性能改进有帮助的。

分词开放测试所追求的高性能的实质是什么？我理解为，我们需要更多的异质的切分语料。本质上来说，机器学习工具在记忆所训练的语料，它只能在和训练语料中出现过的相似度很高的句子上完成正确切分。由于机器学习技术的进步实在有限，其推广能力并不出众。所以，只要能集成进去如果多的额外语料，不管用什么方法。 能够让机器学习工具记住曾有这样的句子，曾经有这样的切分，那么我们就能获得足够高的性能。

关于词典，我要说，额外的词典或许是起正面作用的。我一直认为，不在于是否使用词典，而在于使用什么样的词典。或者说，我们要用那些“好”的词典。词典并非越大越好，也不是所有的词典能够改进分词性 能。有很多这样的负面的例子，很遗憾，我们不能发表这些结果，因为所有的出版出来的学术文章都只能是正面的，积极的结果（或许也是作者选择或投审稿人所 的结果）。关于陷阱，的文章实在太难发了。
（完）


### 参考文献

1. [[转载]对于SIGHAN bakeoff-3的简单综述 ](http://www.newsmth.net/nForum/#!article/NLP/472?p=1#a8)
2. [对于bakeoff-3的简单综述-2](http://guoxinmiao8.blog.sohu.com/67998198.html)
2. [对于bakeoff-3的简单综述-3(完)](http://guoxinmiao8.blog.sohu.com/68000920.html)
